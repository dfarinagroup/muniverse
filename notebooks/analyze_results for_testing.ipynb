{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd26c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "import re       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac24ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install corner"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1815cc6d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "dataset = 'neuromotion_test'\n",
    "bids_root = '../report_cards/'\n",
    "pipeline = 'cbss'\n",
    "path = bids_root + dataset + '-' + pipeline + '/' + 'neuromotion-test' + '-' + pipeline + '_'\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(path + 'report_card_globals_0_50.tsv', delimiter='\\t')\n",
    "df2 = pd.read_csv(path + 'report_card_globals_50_100.tsv', delimiter='\\t')\n",
    "df3 = pd.read_csv(path + 'report_card_globals_100_150.tsv', delimiter='\\t')\n",
    "df4 = pd.read_csv(path + 'report_card_globals_150_200.tsv', delimiter='\\t')\n",
    "df5 = pd.read_csv(path + 'report_card_globals_200_250.tsv', delimiter='\\t')\n",
    "df6 = pd.read_csv(path + 'report_card_globals_250_300.tsv', delimiter='\\t')\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index=True)\n",
    "path = bids_root + dataset + '-' + pipeline + '/'\n",
    "df.to_csv(path + 'report_card_globals.tsv', sep='\\t', index=False)\n",
    "\n",
    "path = bids_root + dataset + '-' + pipeline + '/' + 'neuromotion-test' + '-' + pipeline + '_'\n",
    "df1 = pd.read_csv(path + 'report_card_sources_0_50.tsv', delimiter='\\t')\n",
    "df2 = pd.read_csv(path + 'report_card_sources_50_100.tsv', delimiter='\\t')\n",
    "df3 = pd.read_csv(path + 'report_card_sources_100_150.tsv', delimiter='\\t')\n",
    "df4 = pd.read_csv(path + 'report_card_sources_150_200.tsv', delimiter='\\t')\n",
    "df5 = pd.read_csv(path + 'report_card_sources_200_250.tsv', delimiter='\\t')\n",
    "df = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)\n",
    "path = bids_root + dataset + '-' + pipeline + '/'\n",
    "df.to_csv(path + 'report_card_sources.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f661a94",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "dataset = 'neuromotion_test'\n",
    "bids_root = '../report_cards/'\n",
    "pipeline = 'upperbound'\n",
    "path = bids_root + dataset + '-' + pipeline + '/' + 'neuromotion-test' + '-' + pipeline + '_'\n",
    "\n",
    "# Get all report card globals files\n",
    "import glob\n",
    "all_files = sorted(glob.glob(path + 'report_card_globals_*.tsv'))\n",
    "\n",
    "# Read and combine all dataframes\n",
    "dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file, delimiter='\\t')\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "path = bids_root + dataset + '-' + pipeline + '/'\n",
    "df.to_csv(path + 'report_card_globals.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Do the same for sources files\n",
    "path = bids_root + dataset + '-' + pipeline + '/' + 'neuromotion-test' + '-' + pipeline + '_'\n",
    "all_files = sorted(glob.glob(path + 'report_card_sources_*.tsv'))\n",
    "\n",
    "dfs = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file, delimiter='\\t')\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "path = bids_root + dataset + '-' + pipeline + '/'\n",
    "df.to_csv(path + 'report_card_sources.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc21f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make a helper function that extracts the n-th number from a string\n",
    "\"\"\"\n",
    "def extract_nth_number(s, n):\n",
    "    numbers = re.findall(r'\\d+', s)\n",
    "    if 0 < n <= len(numbers):\n",
    "        return int(numbers[n - 1])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d2f161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Import all report cards\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Path to the report cards\n",
    "bids_root = '../report_cards/'\n",
    "\n",
    "# List of datasetnames\n",
    "# datasetnames = ['Caillet_et_al_2023', 'Grison_et_al_2025', 'Avrillon_et_al_2024', 'hybrid_tibialis', 'neuromotion_test']\n",
    "datasetnames = ['hybrid_tibialis', 'neuromotion_test']\n",
    "\n",
    "# List of algorithm names\n",
    "pipelinenames = ['cbss', 'scd']\n",
    "pipelinenames = ['cbss', 'upperbound']\n",
    "pipelinenames = ['cbss', 'scd', 'upperbound']\n",
    "\n",
    "# Make an empty template of a report card dictonary\n",
    "global_rc = {pipeline: pd.DataFrame() for pipeline in pipelinenames}\n",
    "source_rc = {pipeline: pd.DataFrame() for pipeline in pipelinenames}\n",
    "\n",
    "# Read in report cards\n",
    "for dataset in datasetnames:\n",
    "    for pipeline in pipelinenames:\n",
    "        path = bids_root + dataset + '-' + pipeline + '/'\n",
    "        dataset_global_rc = pd.read_csv(path + 'report_card_globals.tsv', delimiter='\\t')\n",
    "        dataset_source_rc = pd.read_csv(path + 'report_card_sources.tsv', delimiter='\\t')\n",
    "        global_rc[pipeline] = pd.concat([global_rc[pipeline], dataset_global_rc], ignore_index=True)\n",
    "        source_rc[pipeline] = pd.concat([source_rc[pipeline], dataset_source_rc], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems that the only thing we really care about here is the explained variance\n",
    "\n",
    "# From the other table, we get\n",
    "# SIL, PNR, NSPIKES, \n",
    "source_rc['upperbound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64372d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Extract some global metrics from some dataset of interest\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# dataset_of_interest = ['neuromotion-test']\n",
    "dataset_of_interest = ['hybrid-tibialis']\n",
    "#dataset_of_interest = ['Caillet_et_al_2023', 'Grison_et_al_2025', 'Avrillon_et_al_2024']\n",
    "#dataset_of_interest = ['Caillet_et_al_2023', 'Grison_et_al_2025', 'hybrid-tibialis', 'neuromotion-test']\n",
    "\n",
    "task_keys = ['isometric', 'trapezoid']\n",
    "# task_keys = []\n",
    "\n",
    "rows = [f\"{d}-{a}\" for d, a in product(dataset_of_interest, pipelinenames)]\n",
    "columns = ['n_source_10', 'n_source_50', 'n_source_90', \n",
    "           'exp_var_10', 'exp_var_50', 'exp_var_90',\n",
    "           'runtime_10', 'runtime_50', 'runtime_90']\n",
    "summary = pd.DataFrame(np.nan, index=rows, columns=columns) \n",
    "\n",
    "sil_th = 0.87\n",
    "min_num_spikes = 30\n",
    "mvc_threshold = 100\n",
    "\n",
    "for dataset in dataset_of_interest:\n",
    "    for pipeline in pipelinenames:\n",
    "        # Get a unqiue identifier, i.e., dataset-pipeline\n",
    "        row = f\"{dataset}-{pipeline}\"\n",
    "        \n",
    "        # Make a copy of the global data frame\n",
    "        gdf = global_rc[pipeline].copy()\n",
    "        # Only keep entries belonging to the current dataset\n",
    "        gdf = gdf[(gdf['datasetname'] == dataset)]\n",
    "        # Make sure the explained variance is in [0 1]\n",
    "        gdf['explained_var'] = gdf['explained_var'].clip(lower=0, upper=1)\n",
    "        # Focus on specific tasks and mvc level\n",
    "        for task_key in task_keys:\n",
    "            gdf = gdf[gdf['filename'].str.contains(task_key, case=False, na=False)]\n",
    "        # Extract the MVC level from the file name and filter by intensity\n",
    "        if dataset=='neuromotion-test':\n",
    "            mvc_num = 3\n",
    "        else:\n",
    "            mvc_num = 2\n",
    "        gdf['mvc'] = gdf['filename'].apply(lambda x: extract_nth_number(x, mvc_num))    \n",
    "        gdf = gdf[(gdf['mvc'] < mvc_threshold)]\n",
    "        \n",
    "        # Make a copy of the source report card\n",
    "        sdf = source_rc[pipeline].copy()\n",
    "        # Only keep entries belonging to the current dataset\n",
    "        sdf = sdf[(sdf['datasetname'] == dataset)]\n",
    "        # Focus on specific tasks and mvc level\n",
    "        for task_key in task_keys:\n",
    "            sdf = sdf[sdf['filename'].str.contains(task_key, case=False, na=False)]\n",
    "        # Extract the MVC level from the file name and filter by intensity\n",
    "        sdf['mvc'] = sdf['filename'].apply(lambda x: extract_nth_number(x, mvc_num))    \n",
    "        sdf = sdf[(sdf['mvc'] < mvc_threshold)]\n",
    "\n",
    "        # List all filenames that are left to analyze and loop around them   \n",
    "        files = list(gdf['filename'])\n",
    "        n_sources = np.zeros(len(files))\n",
    "        for idx, file in enumerate(files):\n",
    "            # Make a copy of your source datafile \n",
    "            ssdf = sdf.copy()\n",
    "            # Only keep sources that belong to the current file and fullfile some quality checks\n",
    "            ssdf = sdf[(sdf['filename'] == file) & \n",
    "                       (sdf['sil'] > sil_th) & \n",
    "                       (sdf['n_spikes'] > min_num_spikes)]\n",
    "            n_sources[idx] = ssdf.shape[0]\n",
    "\n",
    "        # Get the 10th, 50th and 90th percentile of the number of extracted sources\n",
    "        summary.loc[row, 'n_source_50'] = np.round(np.percentile(n_sources,50) ,2)\n",
    "        summary.loc[row, 'n_source_10'] = np.round(np.percentile(n_sources,10), 2)\n",
    "        summary.loc[row, 'n_source_90'] = np.round(np.percentile(n_sources,90), 2)   \n",
    "        # Get the 10th, 50th and 90th percentile of the runtimes\n",
    "        summary.loc[row, 'runtime_50'] = np.round(np.percentile(gdf['runtime'],50), 0)\n",
    "        summary.loc[row, 'runtime_10'] = np.round(np.percentile(gdf['runtime'],10), 0)\n",
    "        summary.loc[row, 'runtime_90'] = np.round(np.percentile(gdf['runtime'],90), 0)\n",
    "        # Get the 10th, 50th and 90th percentile of the explained variance\n",
    "        summary.loc[row, 'exp_var_50'] = np.round(np.percentile(gdf['explained_var'],50), 2)\n",
    "        summary.loc[row, 'exp_var_10'] = np.round(np.percentile(gdf['explained_var'],10), 2)\n",
    "        summary.loc[row, 'exp_var_90'] = np.round(np.percentile(gdf['explained_var'],90), 2) \n",
    "\n",
    "print(summary)          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdaf75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do spike matching for some dataset of interest\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Dataset of interest\n",
    "#dataset_of_interest = ['Grison_et_al_2025']\n",
    "# dataset_of_interest = ['neuromotion-test']\n",
    "dataset_of_interest = ['hybrid-tibialis']\n",
    "# Pipelines to be analyzed\n",
    "# pipelinenames = ['cbss', 'scd']\n",
    "pipelinenames = ['cbss', 'scd','upperbound']\n",
    "\n",
    "filt_source_rc = {pipeline: pd.DataFrame() for pipeline in pipelinenames}\n",
    "\n",
    "# Make an empty data frame\n",
    "rows = [f\"{d}-{a}\" for d, a in product(dataset_of_interest, pipelinenames)]\n",
    "columns = ['n_source', 'n_rel', \n",
    "           'roa_10', 'roa_50', 'roa_90', \n",
    "           'precision_10', 'precision_50', 'precision_90',\n",
    "           'sensitivity_10', 'sensitivity_50', 'sensitivity_90',\n",
    "           'F1_10', 'F1_50', 'F1_90' \n",
    "           ]\n",
    "summary = pd.DataFrame(np.nan, index=rows, columns=columns) \n",
    "\n",
    "# Select specific tasks\n",
    "# task_keys = []\n",
    "task_keys = ['isometric', 'trapezoid']\n",
    "# Filter by mvc level\n",
    "mvc_threshold = 100\n",
    "sil_th = 0.85\n",
    "match_th = 0.3\n",
    "\n",
    "for dataset in dataset_of_interest:\n",
    "    for pipeline in pipelinenames:\n",
    "        # Get a unqiue identifier, i.e., dataset-pipeline\n",
    "        row = f\"{dataset}-{pipeline}\"\n",
    "        \n",
    "        # Make a copy of the global data frame\n",
    "        gdf = global_rc[pipeline].copy()\n",
    "        # Only keep entries belonging to the current dataset\n",
    "        gdf = gdf[(gdf['datasetname'] == dataset)]\n",
    "        # Make sure the explained variance is in [0 1]\n",
    "        gdf['explained_var'] = gdf['explained_var'].clip(lower=0, upper=1)\n",
    "        # Focus on specific tasks and mvc level\n",
    "        for task_key in task_keys:\n",
    "            gdf = gdf[gdf['filename'].str.contains(task_key, case=False, na=False)]\n",
    "        # Extract the MVC level from the file name and filter by intensity\n",
    "        if dataset=='neuromotion-test':\n",
    "            mvc_num = 3\n",
    "        else:\n",
    "            mvc_num = 2\n",
    "        gdf['mvc'] = gdf['filename'].apply(lambda x: extract_nth_number(x, mvc_num))    \n",
    "        gdf = gdf[(gdf['mvc'] < mvc_threshold)]\n",
    "\n",
    "        # Make a copy of the source report card\n",
    "        sdf = source_rc[pipeline].copy()\n",
    "        # Only keep entries belonging to the current dataset\n",
    "        sdf = sdf[(sdf['datasetname'] == dataset)]\n",
    "        # Focus on specific tasks and mvc level\n",
    "        for task_key in task_keys:\n",
    "            sdf = sdf[sdf['filename'].str.contains(task_key, case=False, na=False)]\n",
    "        # Extract the MVC level from the file name and filter by intensity\n",
    "        sdf['mvc'] = sdf['filename'].apply(lambda x: extract_nth_number(x, mvc_num))    \n",
    "        sdf = sdf[(sdf['mvc'] < mvc_threshold)]\n",
    "        # Only keep sources that belong to the current file and fullfile some quality checks\n",
    "        sdf = sdf[(sdf['sil'] > sil_th)]\n",
    "        \n",
    "        # Get the total number of sources\n",
    "        n_total = int(sdf.shape[0])\n",
    "        # Get the number of matched sources\n",
    "        sdf = sdf[(sdf['TP'] > 1)]\n",
    "        # Compute the rate of agreement\n",
    "        sdf['RoA'] = sdf['TP'] / (sdf['TP'] + sdf['FP'] + sdf['FN'])\n",
    "        # Compute the precision\n",
    "        sdf['Precision'] = sdf['TP'] / (sdf['TP'] + sdf['FP'])\n",
    "        # Compute the recall\n",
    "        sdf['Recall'] = sdf['TP'] / (sdf['TP'] + sdf['FN'])\n",
    "        # Calculate the F1 score\n",
    "        sdf['F1'] = 2*sdf['Precision']*sdf['Recall']/ (sdf['Precision'] + sdf['Recall'])\n",
    "\n",
    "        sdf = sdf[(sdf['Recall']>match_th)]\n",
    "\n",
    "        filt_source_rc[pipeline] = sdf\n",
    "\n",
    "        # Get both total and relative number of matched sources\n",
    "        summary.loc[row, 'n_source'] = int(sdf.shape[0])\n",
    "        if n_total > 0:\n",
    "            summary.loc[row, 'n_rel'] = int(sdf.shape[0]) / n_total \n",
    "        else:\n",
    "            summary.loc[row, 'n_rel'] = 0\n",
    "        # Get the 10th, 50th and 90th percentile of the RoA\n",
    "        summary.loc[row, 'roa_10'] = np.round(np.percentile(sdf['RoA'],10),2)\n",
    "        summary.loc[row, 'roa_50'] = np.round(np.percentile(sdf['RoA'],50),2)\n",
    "        summary.loc[row, 'roa_90'] = np.round(np.percentile(sdf['RoA'],90),2)\n",
    "        # Get the 10th, 50th and 90th percentile of the Precision\n",
    "        summary.loc[row, 'precision_10'] = np.round(np.percentile(sdf['Precision'],10),2)\n",
    "        summary.loc[row, 'precision_50'] = np.round(np.percentile(sdf['Precision'],50),2)\n",
    "        summary.loc[row, 'precision_90'] = np.round(np.percentile(sdf['Precision'],90),2)\n",
    "        # Get the 10th, 50th and 90th percentile of the Recall\n",
    "        summary.loc[row, 'sensitivity_10'] = np.round(np.percentile(sdf['Recall'],10),2)\n",
    "        summary.loc[row, 'sensitivity_50'] = np.round(np.percentile(sdf['Recall'],50),2)\n",
    "        summary.loc[row, 'sensitivity_90'] = np.round(np.percentile(sdf['Recall'],90),2)\n",
    "        # Get the 10th, 50th and 90th percentile of the F1 score\n",
    "        summary.loc[row, 'F1_10'] = np.round(np.percentile(sdf['F1'], 10),2)\n",
    "        summary.loc[row, 'F1_50'] = np.round(np.percentile(sdf['F1'], 50),2)\n",
    "        summary.loc[row, 'F1_90'] = np.round(np.percentile(sdf['F1'], 90),2) \n",
    "\n",
    "\n",
    "\n",
    "print(summary)          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a63ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to extract movement type from filename\n",
    "def extract_movement_type(filename):\n",
    "    filename = filename.lower()\n",
    "    if 'isometric' in filename and 'trapezoid' in filename:\n",
    "        return 'iso_trapezoid'\n",
    "    elif 'isometric' in filename and 'triangular' in filename:\n",
    "        return 'iso_triangular'\n",
    "    elif 'isometric' in filename and 'sinusoid' in filename:\n",
    "        return 'iso_sinusoidal'\n",
    "    elif 'isometric' in filename and 'ballistic' in filename:\n",
    "        return 'iso_ballistic'\n",
    "    elif 'dynamic' in filename and 'sinusoid' in filename:\n",
    "        return 'dyn_sinusiod'\n",
    "    elif 'dynamic' in filename and 'triangular' in filename:\n",
    "        return 'dyn_triangular'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Quality thresholds\n",
    "sil_th = 0.87\n",
    "min_num_spikes = 30\n",
    "\n",
    "# Create a new dataframe with the metrics we want to analyze\n",
    "metrics_df = pd.DataFrame()\n",
    "\n",
    "for pipeline in pipelinenames:\n",
    "    # Get global metrics\n",
    "    gdf = global_rc[pipeline].copy()\n",
    "    gdf['movement_type'] = gdf['filename'].apply(extract_movement_type)\n",
    "    gdf['pipeline'] = pipeline\n",
    "    \n",
    "    # Get source metrics\n",
    "    sdf = source_rc[pipeline].copy()\n",
    "    sdf['movement_type'] = sdf['filename'].apply(extract_movement_type)\n",
    "    \n",
    "    # Calculate number of sources per file that meet quality criteria\n",
    "    n_sources = []\n",
    "    for file in gdf['filename']:\n",
    "        ssdf = sdf[(sdf['filename'] == file) & \n",
    "                  (sdf['sil'] > sil_th) & \n",
    "                  (sdf['n_spikes'] > min_num_spikes)]\n",
    "        n_sources.append(ssdf.shape[0])\n",
    "    \n",
    "    gdf['n_sources'] = n_sources\n",
    "    \n",
    "    # Calculate average SIL and peak height per file\n",
    "    avg_metrics = sdf.groupby('filename').agg({\n",
    "        'sil': 'mean',\n",
    "        'peak_height': 'mean',\n",
    "        'movement_type': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Merge with global metrics\n",
    "    merged_df = pd.merge(gdf, avg_metrics, on=['filename', 'movement_type'])\n",
    "    \n",
    "    # Select and rename columns\n",
    "    metrics_df = pd.concat([metrics_df, merged_df[['pipeline', 'movement_type', \n",
    "                                                  'explained_var', 'n_sources',\n",
    "                                                  'sil', 'peak_height']]])\n",
    "\n",
    "# Create subplots for each metric\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))  # Made figure larger to accommodate more categories\n",
    "fig.suptitle('Metrics by Movement Type and Pipeline', fontsize=16)\n",
    "\n",
    "# Plot explained variance\n",
    "sns.boxplot(data=metrics_df, x='movement_type', y='explained_var', hue='pipeline', ax=axes[0,0])\n",
    "axes[0,0].set_title('Explained Variance')\n",
    "axes[0,0].set_xlabel('Movement Type')\n",
    "axes[0,0].set_ylabel('Explained Variance')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)  # Rotate x-axis labels for better readability\n",
    "axes[0,0].set_ylim(0,1)\n",
    "\n",
    "# Plot number of sources\n",
    "sns.boxplot(data=metrics_df, x='movement_type', y='n_sources', hue='pipeline', ax=axes[0,1])\n",
    "axes[0,1].set_title('Number of Sources')\n",
    "axes[0,1].set_xlabel('Movement Type')\n",
    "axes[0,1].set_ylabel('Number of Sources')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot average SIL\n",
    "sns.boxplot(data=metrics_df, x='movement_type', y='sil', hue='pipeline', ax=axes[1,0])\n",
    "axes[1,0].set_title('Average SIL')\n",
    "axes[1,0].set_xlabel('Movement Type')\n",
    "axes[1,0].set_ylabel('Average SIL')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot average peak height\n",
    "sns.boxplot(data=metrics_df, x='movement_type', y='peak_height', hue='pipeline', ax=axes[1,1])\n",
    "axes[1,1].set_title('Average Peak Height')\n",
    "axes[1,1].set_xlabel('Movement Type')\n",
    "axes[1,1].set_ylabel('Average Peak Height')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics by Movement Type and Pipeline:\")\n",
    "summary = metrics_df.groupby(['movement_type', 'pipeline']).agg({\n",
    "    'explained_var': ['mean', 'std'],\n",
    "    'n_sources': ['mean', 'std'],\n",
    "    'sil': ['mean', 'std'],\n",
    "    'peak_height': ['mean', 'std']\n",
    "}).round(3)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40b897ff",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sdf1 = filt_source_rc['upperbound']\n",
    "sdf1['alg'] = 'upperbound'\n",
    "x1 = np.asarray(sdf1['peak_height'])\n",
    "y1 = np.asarray(sdf1['mean_dr'])\n",
    "\n",
    "sdf2 = filt_source_rc['cbss']\n",
    "sdf2['alg'] = 'cbss'\n",
    "x2 = np.asarray(sdf2['peak_height'])\n",
    "y2 = np.asarray(sdf2['mean_dr'])\n",
    "\n",
    "sdf3 = filt_source_rc['scd']\n",
    "sdf3['alg'] = 'scd'\n",
    "x3 = np.asarray(sdf3['peak_height'])\n",
    "y3 = np.asarray(sdf3['mean_dr'])\n",
    "\n",
    "df = pd.concat([sdf1, sdf2, sdf3], ignore_index=True)\n",
    "\n",
    "sns.pairplot(df, kind='hist' ,hue='alg', vars=['mean_dr', 'peak_height', 'sil', 'unit_id_ref'])\n",
    "\n",
    "# # Create a JointGrid\n",
    "# g = sns.JointGrid(x=x1, y=y1)\n",
    "\n",
    "# # Plot first dataset\n",
    "# g.plot_joint(sns.scatterplot, color=\"blue\", label=\"ub\",s=30, alpha=0.4)\n",
    "# sns.histplot(x=x1, color=\"blue\", ax=g.ax_marg_x, alpha=0.8, bins=np.linspace(0,40,31))\n",
    "# sns.histplot(y=y1, color=\"blue\", ax=g.ax_marg_y, alpha=0.8, bins=np.linspace(5,45,31))\n",
    "\n",
    "# # Overlay third dataset\n",
    "# sns.scatterplot(x=x3, y=y3, color=\"green\", ax=g.ax_joint, label=\"scd\", s=20, alpha=0.2)\n",
    "# sns.histplot(x=x3, color=\"green\", ax=g.ax_marg_x, alpha=0.6, bins=np.linspace(0,40,31))\n",
    "# sns.histplot(y=y3, color=\"green\", ax=g.ax_marg_y, alpha=0.6, bins=np.linspace(5,45,31))\n",
    "\n",
    "# # Overlay second dataset\n",
    "# sns.scatterplot(x=x2, y=y2, color=\"red\", ax=g.ax_joint, label=\"cbss\", s=10, alpha=0.4)\n",
    "# sns.histplot(x=x2, color=\"red\", ax=g.ax_marg_x, alpha=0.4, bins=np.linspace(0,40,31))\n",
    "# sns.histplot(y=y2, color=\"red\", ax=g.ax_marg_y, alpha=0.4, bins=np.linspace(5,45,31))\n",
    "\n",
    "\n",
    "\n",
    "# # Add legend\n",
    "# g.ax_joint.legend()\n",
    "# g.ax_joint.set_xlim(0,40)\n",
    "# g.ax_joint.set_xlabel(\"Expected Peak hight\")\n",
    "# g.ax_joint.set_ylabel(\"Mean firing rate (Hz)\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import corner\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data\n",
    "sdf1 = filt_source_rc['upperbound']\n",
    "sdf2 = filt_source_rc['cbss']\n",
    "sdf3 = filt_source_rc['scd']\n",
    "\n",
    "# Variables to plot\n",
    "variables = ['mean_dr', 'peak_height', 'sil', 'unit_id_ref', 'F1']\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Plot each algorithm with a different color\n",
    "colors = ['C0', 'C3', 'C2']\n",
    "algorithms = ['upperbound', 'cbss', 'scd']\n",
    "datasets = [sdf1, sdf2, sdf3]\n",
    "\n",
    "# First plot to set up the corner plot\n",
    "corner.corner(datasets[0][variables].values,\n",
    "             labels=variables,\n",
    "             color=colors[0],\n",
    "             alpha=0.8,\n",
    "             hist_kwargs={'density': True},\n",
    "             fig=fig,\n",
    "             plot_density=False,\n",
    "             plot_contours=False,\n",
    "             plot_datapoints=True,\n",
    "             markersize=4)\n",
    "\n",
    "# Overlay the other datasets\n",
    "for data, color, label in zip(datasets[1:], colors[1:], algorithms[1:]):\n",
    "    corner.corner(data[variables].values,\n",
    "                 color=color,\n",
    "                 alpha=0.8,\n",
    "                 hist_kwargs={'density': True},\n",
    "                 fig=fig,\n",
    "                 plot_density=False,\n",
    "                 plot_contours=False,\n",
    "                 plot_datapoints=True,\n",
    "                 markersize=4)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(algorithms, loc='upper right', bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a7901b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
